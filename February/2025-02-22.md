## ğŸ“… ë‚ ì§œ: 2025-02-22

### ğŸ’¬ ìŠ¤í¬ëŸ¼
- í•™ìŠµ ëª©í‘œ 1 : langchain ì‚¬ìš©ë²• ê³µë¶€
- í•™ìŠµ ëª©í‘œ 2 : llm ì—°ë™ ê³µë¶€
  
### ğŸ“’ ê³µë¶€í•œ ë‚´ìš©
---
ì¤€ë¹„ :

- OpenAI API Key ë°œê¸‰
- Tavily API Key ë°œê¸‰
<br><br>
ë­ì²´ì¸

https://python.langchain.com/docs/introduction/#tutorials

ì„ íƒ: ë­ìŠ¤ë¯¸ìŠ¤

---

ê¸°ë³¸ ê°œë… : ì½”ë“œë¡œ í”„ë¡¬í”„íŒ…ì„ í•œë‹¤.
<br><br>
LangChain

- LLM ê°œë°œ ì• í”Œë¦¬ì¼€ì´ì…˜ framework
<br><br>
ë­ì²´ì¸ ì„¤ì¹˜ : pip install langchain

ëª¨ë¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ : pip install -qU "langchain[openai]â€

í™˜ê²½ ë³€ìˆ˜ : ìš´ì˜ì²´ì œ ìˆ˜ì¤€ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒ. â€œ.envâ€ íŒŒì¼ì— ì €ì¥

- gitignoreì— .env ì¶”ê°€

```python
OPENAI_API_KEY="YOUR_KEY"
TAVILY_API_KEY="YOUR_KEY"
```
<br><br>
ìœ ë‹‰ìŠ¤ ì‰˜(í„°ë¯¸ë„)

- export LANGSMITH_TRACING="true"
- export LANGSMITH_API_KEY="..."
- set LANGSMITH_TRACING="true"
- (ìœ„ 3ì¤„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, ëŒ€ì‹  .env ì”€)
<br><br>
app.py

```python
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```
<br><br>
pip install dotenv :  env íŒŒì¼ ì½ì–´ë“¤ì„

```cpp
from dotenv import load_dotenv

load_dotenv()
```

---

ì„œë²„ (API) í™”
<br><br>
GET ìš”ì²­ì— ë¶€ê°€ ì •ë³´ë¡œ ê²½ë¡œ íŒŒë¼ë¯¸í„° ì™¸ì— ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤.

ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: ê²½ë¡œ ë’¤ì— ?key1=value1&key2=value2

ì˜ˆ) https://â€¦../say?text=hi
<br><br>
ì„œë²„ ì‹¤í–‰ : fastapi dev server.py

ë°±ì•¤ë“œ, í”„ë¡ íŠ¸ì™€ ì†Œí†µí•  ë•Œ:

- ì˜ˆì‹œ) GETìœ¼ë¡œ ê²½ë¡œëŠ” translate, ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜ëŠ” textì™€ languageë¡œ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤

---

ì±—ë´‡

ë­ê·¸ë˜í”„ = ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆìŒ
<br><br>
pip install langchain-core langgraph>0.2.27

https://python.langchain.com/docs/tutorials/chatbot/
<br><br>
thread idë¡œ ëŒ€í™” êµ¬ë¶„ (ì—¬ëŸ¬ëª… ê°ê° ëŒ€í™” êµ¬ë¶„ ê°€ëŠ¥)

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define the function that calls the model
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}

# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {"configurable": {"thread_id": "abc123"}}

query = "Hi! I'm Bob."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # output contains all messages in state
```

