## 📅 날짜: 2025-02-14

### 💬 스크럼
- 학습 목표 1 : 딥다이브 공부
  
### 📒 공부한 내용
### 딥다이브(오전)

주제 : 데이터 리모델링 패키지를 활용해 데이터를 변환하고 집계하는 방법을 설명하시오.<br><br>

→ 한 줄 요약 :

pandas를 활용해 데이터를 pivot, melt 등으로 변환(reshape)하고, 
groupby와 agg 함수를 이용해 집계(aggregation)하여 요약 통계 정보를 도출한다.<br><br>

데이터 리모델링이란?

- 원시 형태의 데이터를 분석하기 쉽도록 재구조화(reshaping) 하고, 요약/집계 (aggregation) 하는 과정
- 예) 행렬 형태 2차원 테이블을 다시 길게, 혹은 넓게 만드는 것 - 재구조화, 즉 변환
- 예) 여러 열을 기준으로 합계, 평균 등 통계량을 구해 정리하는 것 - 집계<br><br>

데이터 리모델링 패키지

- 파이썬 : pandas 라이브러리
    - 추가로 pyspark(분산처리 특화), Dask(병렬처리 지원)
    - 둘다 pandas와 유사한 dataFrame API제공
    - pyspark
        
        ```python
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col, avg, sum
        
        # Spark 세션 생성
        spark = SparkSession.builder.appName("DataRemodelingExample").getOrCreate()
        
        # 예시 데이터 생성
        data = [
            ("A", 10), ("B", 20), ("A", 30), ("B", 40), ("A", 50)
        ]
        columns = ["category", "value"]
        df = spark.createDataFrame(data, columns)
        
        # 그룹화 및 집계
        df_grouped = df.groupBy("category").agg(
            avg(col("value")).alias("avg_value"),
            sum(col("value")).alias("sum_value")
        )
        df_grouped.show()
        ```
        
    - Dask
        
        ```python
        import dask.dataframe as dd
        
        # CSV 파일을 읽어 Dask DataFrame 생성 (예시)
        ddf = dd.read_csv("large_dataset.csv")
        
        # groupby를 통한 집계 작업
        result = ddf.groupby("category")["value"].mean().compute()
        print(result)
        ```
        
- **R :** tidyverse(dplyr, tidyr)와 data.table을 통해 직관적이고 강력한 데이터 조작 기능을 제공
- **SQL**은 데이터베이스 내에서 바로 피벗과 그룹 집계 수행

---

변환과 집계<br><br>

데이터 변환이란? → reshaping

- 데이터의 구조, 즉 형태 자체를 바꾸는 작업
    - 넓은 의미에서는 데이터를 원하는 상태로 가공하기 위한 모든 연산
- 목적 : 분석, 시각화, 피벗테이블 등에 편리한 형태로 재배열
- 예) wide-long 변환
- 예) 행-열 인덱스 재배치(피벗)<br><br>

넓은 의미(선택과 정제 등이 넓은 의미에서 포함):

- 필터링 : 변환이라기 보단 조작(데이터 일부 선택이므로)
- 중복 제거 : 변환 아니고 데이터 정제
- merging : 데이터 통합, 조합<br><br>

그렇다면 스케일링, 정규화, 로그 변환, 인코딩 등은?

- 데이터 구조가 아닌 데이터 값 변경에 초점을 둔 데이터 전처리, 혹은 특성 변환이다
- 역시 넓은 의미에서 데이터 변환의 일종이긴 함
- 종류
    
    스케일링 및 정규화
    
    - 표준화
        - 정규분포 따르도록 평균0 분산1로 변환
        - 선형 회귀, 로지스틱 회귀 등 사용
    - 최소-최대 정규화
        - 데이터 범위를 [0, 1] 혹은 [-1, 1]로 조정
        - 신경망 모델에서 주로 사용, 데이터의 상대적 크기 유지
        - 특성값(feature value)의 범위를 조정해, 모델이 편향되지 않고 수렴을 빠르게 하도록 돕는 것<br><br>
    
    로그 변환
    
    - 로그 변환
        - 데이터 분포가 오른쪽 치우친 경우 로그 함수 적용하여 정규화
        - 금융 데이터, 인구 통계 데이터<br><br>
    
    인코딩
    
    - 범주형 변수 인코딩
        - **One-Hot Encoding:** 각 범주를 이진 벡터로 변환
        - **Label Encoding:** 각 범주에 정수 값을 할당
        - **Target Encoding:** 범주별로 타겟 변수의 평균을 할당
        - **적용 상황:** 머신러닝 모델에 입력하기 전 범주형 데이터를 수치형으로 변환할 때 필수적.<br><br>

집계란?

- 개별 데이터를 요약하거나 압축해서 의미 있는 정보를 산출하는 과정
- 보통 그룹으로 묶어 합계, 평균, 최댓값, 최솟값 등 계산
- 예) groupby함수 사용<br><br>

---

### 딥다이브(오후)<br><br>

데이터 : 영화 매출 데이터

코드 내용 :

- 집계 : 상영관과 영화별 매출 총합
- 집계 : 일 단위로 **매출 총합**
- 변환 : 데이터를 wide form ↔ long form으로 변환
- 변환 : stack과 unstack 사용하여 변환<br><br>

코드

```python
import pandas as pd
import numpy as np
pd.set_option('display.float_format', lambda x: '%.0f' % x)  # 과학적 표기법(e등) 말고 정수 형태로 출력하는 코드

# --------------------------------------------------------
# 1) 영화 데이터
# --------------------------------------------------------
# 날짜 범위 (2025-02-14 ~ 2025-02-20, 7일)
dates = pd.date_range('2025-02-14', periods=7)

# 영화 3종, 상영관 3종
movie_names = ['Memento', 'Inception', 'Tenet'] 
theaters = ['CGV', 'Lotte cinema', 'Megabox']

# 랜덤 데이터 생성
records = []
for d in dates:
    for m in movie_names:
        for t in theaters:
            ticket_sold = np.random.randint(500, 2000)  # 500 ~ 2000장 사이 랜덤
            revenue = ticket_sold * 10000                # 티켓 1장당 10000원
            records.append([d, m, t, ticket_sold, revenue])

df = pd.DataFrame(records, columns=['date', 'movie_names', 'theater', 'ticket_sold', 'revenue'])

# --------------------------------------------------------
# 2) 시계열 데이터 변환
# --------------------------------------------------------
# date를 시계열로 변환
df['date'] = pd.to_datetime(df['date'])

print(">> 원본 데이터프레임 확인 <<")
print(df, "\n")

# 집계
# --------------------------------------------------------
# 3) GroupBy를 이용한 집계
#    - theater, movie_names별 수익 합계
# --------------------------------------------------------
grouped = df.groupby(['theater', 'movie_names'])['revenue'].sum().reset_index()
print(">> 영화관 & 영화별 총 매출 <<")
print(grouped, "\n")

# --------------------------------------------------------
# 4) 날짜를 인덱스로 지정 후 일 단위 resample
# --------------------------------------------------------
df_indexed = df.set_index('date')
df_daily = df_indexed.resample('D')['revenue'].sum()  # 일 단위 집계
print(">> 일 단위 매출 합계 <<")
print(df_daily, "\n")

# 구조 변환 #
# --------------------------------------------------------
# 5) Pivot & Pivot Table
# --------------------------------------------------------
# 날짜(date)와 영화(movie_names)를 함께 index로, 영화관(theater)을 columns로 사용
# 좀 더 보기 편해짐
df_reset = df.reset_index(drop=True)  # 기존 index를 버리고 새 index 부여

df_pivot = df_reset.pivot(
    index=['date', 'movie_names'],
    columns='theater',
    values='revenue'
)
print(">> pivot 결과 <<")
print(df_pivot.head(10), "\n")

# pivot_table(): 중복이 있어도 aggfunc(aggregation function)로 집계 처리 가능
df_pivot_table = pd.pivot_table(
    df_reset,
    index='movie_names',
    columns='theater',
    values='revenue',
    aggfunc=['sum', 'mean', 'max', 'min']
)
print(">> pivot_table 결과 <<")
print(df_pivot_table, "\n")

# --------------------------------------------------------
# 6) Melt: Wide → Long 형태 변환
#    - pivot 결과를 다시 long 형으로
# --------------------------------------------------------
df_melt = pd.melt(
    df_pivot.reset_index(),
    id_vars=['date', 'movie_names'],
    var_name='theater',
    value_name='revenue'
)
print(">> melt 결과 (wide → long) <<")
print(df_melt.head(10), "\n")

# --------------------------------------------------------
# 7) Stack/Unstack 예시 (멀티인덱스)
# --------------------------------------------------------
# date, movie_names, theater를 인덱스로 묶어둔 상태에서 ticket_sold와 revenue만 남김
df_multi = df_reset.set_index(['date', 'movie_names', 'theater'])[['ticket_sold', 'revenue']]
print(">> 멀티 인덱스 DataFrame <<")
print(df_multi.head(9), "\n")

# 7-1) stack() → 열을 행의 하위 레벨로 쌓기
df_stacked = df_multi.stack()
print(">> df_multi.stack() 결과 <<")
print(df_stacked.head(12), "\n")

# 7-2) unstack() → 행의 하위 레벨을 열로 펼치기 (원상복구)
df_unstacked = df_stacked.unstack()
print(">> df_stacked.unstack() 결과 (원상복구) <<")
print(df_unstacked.head(9), "\n")
```
  
### 💭 오늘의 회고
- 성공적인 점 : 집중해서 주제에 관련된 내용을 탐구했음
- 개선해야 할 점 : 더 수준높은 자료(공식 문서 등)를 리서치하고, 완성도를 높여야겠다
  
