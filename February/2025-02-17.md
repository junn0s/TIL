## 📅 날짜: 2025-02-17

### 💬 스크럼
- 학습 목표 1 : 다변수 함수의 미분 강의 필기 및 공부
- 학습 목표 2 : 코딩테스트 공부
  
### 📒 공부한 내용
### 다변수 함수의 미분<br><br>

복습<br><br>

도함수 : x에서의 미분계수를 출력하는 함수

미분계수 : 해당 점에서의 함수의 변화율, 즉 선형 근사

미분 : 미분계수를 구한다 or 도함수를 구한다

미분은 행렬이다

---

공역을 기준으로 하는 함수

- 스칼라 함수
    - 실함수(실수) - 실수가 공역
    - 복소함수
- 벡터 함수<br><br>

정의역을 기준으로 하는 함수

- 일변수
    - 미분계수 : 실수
    - 접선의 방정식 : 선형 근사 (그 점에서 가장 가까운 형태의 1차함수로 만드는 과정)
- 다변수
    - 미분계수 : 함수
    - 편미분 → 해당 변수에 대한 변화율을 나타내는 함수. 즉 편미분들을 모아서 션형 변환 (미분 함수) 구성 가능
    - 미분 함수는 야코비안 행렬로 표현됨. 즉 각 원소는 해당 변수에 대한 편미분
        - 모든 선형 변환은 행렬로 표현 가능하기 때문
    - 선형 변환 대응규칙은 행렬 곱으로 나타낼 수 있음

---

편미분과 방향 미분 (아래는 스칼라 함수 경우)<br><br>

표준 순서 기저를 하나 뽑아서 그 방향으로 미분

$$
\mathbb{R}^2
={(a,b)∣a,b∈R }=span{(1,0),(0,1)}
$$

$f(x, y) = x^2 + y^2$

x에 대해 편미분 = $2x$ (x 이외 값은 상수 취급)

y에 대해 편미분 = $2y$ (y 이외 값은 상수 취급)

$grad$ → $2x, 2y$<br><br>

$f(x, y) = xy$

x에 대해 편미분 = $y$

y에 대해 편미분 = $x$

$grad$ → $y, x$<br><br>

그래디언트(기울기) = 스칼라 함수의 모든 편미분을 모아 구성한 벡터

스칼라 함수 : 한 점에서의 변화율을 열벡터(그래디언트)로 나타냄

벡터 함수 : 각 성분 함수의 변화율(그래디언트)을 모아 야코비안 행렬로 표현<br><br>

야코비안 행렬 : 미분의 다변수 확장 시 표현 (각 변수에 대한 편미분으로 구성된 행렬)

- 단일 변수 함수에서는 선형 근사를 도함수로 표현
- 다변수 함수에서는 그 근사가 선형 변환으로 표현됨
- 이 선형 변환을 야코비안 행렬이 나타냄

---

미분연산자(미분이라는 함수) 는 선형일까?

→ 선형이다(그래프가 직선)

→ 행렬로 표현할 수 있다.

→ 미적분에서 다루는 함수(예를 들어 y = x^2) 는 비선형임, 다만 미분 연산을 적용하면 선형 연산의 결과를 얻음

---

단순 선형 회귀(일변수)

$f(x) = wx + b$ (w = 가중치, b = 절편)

$$
l(w,b)
= \sum_{i=1}^n \bigl(f(x_i) - y_i\bigr)^2
= \sum_{i=1}^n \bigl(w x_i + b - y_i\bigr)^2
$$

(위) 손실 함수 (이 값을 최소화하는 w와 b를 찾아야 함) → 가장 잘 들어맞는 모델

$$
∇l(w,b)=(∑2(wx_i+b−y_i)x_i, ∑2(wx_i+b−y_i))
$$

(위) 손실 함수 편미분. 최소가 되려면 이 값을 0으로 설정해야 함

$$
w = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$

(위) 가중치 구하는 법

---

다중 선형 회귀(다변수)

$$
\quad
y = w_1 x_1 + w_2 x_2 + \dots + w_d x_d + b
$$

행렬로 표현하면, 아래와 같음

$$
X = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1d} \\
x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nd}
\end{bmatrix}, 
\quad
\mathbf{y} 
= \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix},
\quad
\mathbf{w}
= \begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_d
\end{bmatrix}
$$

손실 함수는 아래와 같음

$$
\ell(\mathbf{w}, b)
= \sum_{i=1}^n 
 \Bigl( y_i - \bigl(\mathbf{w} \cdot \mathbf{x}_i + b\bigr) \Bigr)^2.\\
\ell(\mathbf{w}, b)
= \|\mathbf{y} - (X \mathbf{w} + b\mathbf{1})\|^2,
$$

loss function이 최소인 파라미터 (w와 b) 를 찾는 것

$$
\tilde{\mathbf{w}}
= \bigl(\tilde{X}^T \tilde{X}\bigr)^{-1}
  \,\tilde{X}^T\,\mathbf{y}.
$$

---

경사하강법

1. x1 = 아무 입력값 하나 선택
2. f(x1) ≥ f(x2) 인 x2 선택
    1. 접선의 기울기가 양수면 증가함수이므로 음수 쪽으로, 음수면 양수 쪽으로
    2. 즉 그래디언트 방향과 반대로 가야 함 (가장 빠르게 감소하는 방향)<br><br>

하이퍼파라미터 : 모델에 들어있는 변수는 아니지만, 학습에 영향을 줌

- 학습률 : 그래디언트 방향은 알지만, 얼만큼 간격으로 이동해야 할지 정하는 파라미터<br><br>
    

함수 $f(x, y) = x^2 + y^2$ 에 대해 한 번의 경사 하강법(iteration) 예시<br><br>
    
1. **함수와 그래디언트 구하기:**
    
$$
f(x, y) = x^2 + y^2,\quad \nabla f(x,y) =
\begin{pmatrix}
2x \\
2y
\end{pmatrix}
$$

2. **초기값 설정:**
    
초기 점을 \((x_0, y_0) = (1, 2)\)라고 하자

3. **현재 함수 값 계산:**
    
$$
f(1,2) = 1^2 + 2^2 = 1 + 4 = 5
$$

4. **현재 점에서의 그래디언트 계산:**
    
$$
\nabla f(1,2) = \begin{pmatrix} 2 \times 1 \\ 2 \times 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 4 \end{pmatrix}
$$

5. **학습률(learning rate) 설정:**
    
보통 \( \alpha \) 로 표기하며, 예를 들어 \( \alpha = 0.1 \) 라고 하자

6. **업데이트 규칙 적용:**
    
경사 하강법의 업데이트 규칙은

$$
(x_{\text{new}}, y_{\text{new}}) = (x_{\text{old}}, y_{\text{old}}) - \alpha \, \nabla f(x_{\text{old}}, y_{\text{old}})
$$

따라서,

$$
(x_1, y_1) = (1, 2) - 0.1 \cdot \begin{pmatrix} 2 \\ 4 \end{pmatrix} = \begin{pmatrix} 1 - 0.2 \\ 2 - 0.4 \end{pmatrix} = \begin{pmatrix} 0.8 \\ 1.6 \end{pmatrix}
$$

7. **새로운 점에서의 함수 값 계산:**
    
$$
f(0.8,1.6) = (0.8)^2 + (1.6)^2 = 0.64 + 2.56 = 3.2
$$

함수 값이 5에서 3.2로 감소한 것을 확인할 수 있다
        
    
이 과정을 반복하면 함수 값이 계속 줄어들어 최종적으로 f(x, y)의 최소값인 0 (최소점 (0,0)에 수렴하게 됨
    

---

결론 : 미적분(비선형) → 선형 근사(미분) 를 해서 국소적 선형 변환으로 표현 → 이를 행렬로 나타내서 빠르게 계산 가능
<br><br>
미분 : 시간에 따른 변화율을 나타낸 것. 복잡한 비선형 문제를 선형 문제로 근사시켜 행렬로 표현하여 효율적으로 컴퓨터가 계산할 수 있도록 함. 머신러닝에서는 데이터 기반 예측을 위해 모델의 손실 함수를 최소화하는 과정(경사 하강법 등)에서 미분(그래디언트)을 사용해 파라미터를 업데이트함.

수학(알고리즘)은 계산을 줄여준다.
그레디언트 제로(경사 0) -> 그레디언트 디슨트(경사 하강)
DP(Dynamic Programming), 메모이제이션


### 🔥 오늘의 도전 과제
- 도전 과제 1: 팀 스크럼때 나온 코테 7문제 해결
  
### 💭 오늘의 회고
- 성공적인 점 : 다변수 미분과 선형 변환(행렬 표현), 이어서 경사하강법까지 모두 개념이 이어지는 것을 알게 되었다
- 개선해야 할 점 : 컨디션 관리. 잠 8시간 + 매일 운동 1시간은 필수로 해야할 것 같음
  
### 📁 참고 자료 및 링크
- Alex 강의 및 'DMT PARK' 유튜브 - 미분은 행렬이다
