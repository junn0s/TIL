## 📅 날짜: 2025-02-24

### 💬 스크럼
- 학습 목표 1 : ANN, Activation Function, Loss Function, 역전파, 옵티마이저
- 학습 목표 2 : CNN 공부
  
### 📒 공부한 내용
---
### Activation Function (활성화 함수)<br><br>

Activation function (비선형 활성화 함수)

$h(x) = f(g(x))$

g = 가중치 합, f = 활성화 함수 (순서가 중요)<br><br>

비선형 함수 : 전체 구간에서 동일한 기울기를 가지지 않는 함수 (미분 시)

선형 함수 : 1차함수 (전체 구간에서 동일한 기울기를 가짐)<br><br>

종류 : 시그모이드, 렐루, 하이퍼볼릭 탄젠트 등<br><br>

1. 시그모이드 : 모든 값을 0과 1 사이로 매핑하는 S자 모양 함수
    
    단점 : 기울기 소실 문제가 발생할 확률이 높아짐
    
2. 하이퍼볼릭 탄젠트 : 모든 값을 -1과 1 사이로 매핑하는 S자 모양 함수
    
    특징 : 출력 범위가 더 넓어 중심이 0에 가까운 데이터 표현 유리
    
    단점 : 기울기 소실 문제 등
    
3. ReLU : 0보다 작으면 0, 0 이상이면 입력값 그대로 출력하는 함수
    
    장점 : 가장 많이 쓰임. 계산이 간단함, 기울기 소실 완화
    
    단점 : 입력값이 음수면 해당 뉴런은 죽음
    

    
---
### ANN (인공 신경망)

### Artificial Neural Network

- 뇌의 뉴런 네트워크를 모방한 알고리즘
- input layer, hidden layer, output layer로 구성됨
<br><br>

- 가중치 개수 : 2*3 + 3*1 = 9
    - 가중치 계산 : np.dot (행렬 곱)
- hidden layer에서 활성화 함수 사용(ReLU) → 비선형성 추가<br><br>

Feed-Forward(순전파)

- 순서대로 입력 → 출력층까지 전달, 입력을 통해 예측값을 생성하는 과정
- 모델 학습 ‘과정’에 사용
- 학습 과정 중 일부 노드들만 활성화 됨 (ReLU)
- 출력값과 실제 정답을 비교해서 손실값을 계산함<br><br>


Backpropagation(역전파)

- 계산된 손실 값을 기반으로, 업데이트 할 ‘방향’ 을 정해줌
    - 즉 각 가중치에 대한 기울기(그래디언트)를 도함수를 사용해 계산<br><br>
    

Optimizer을 통한 가중치 업데이트

- 방향 뿐 아니라 ‘거리’ 도 포함하여 가중치 업데이트
- 학습률 기준으로 거리가 정해짐
- 예) 경사 하강법<br><br>

---

### Fully connected neural network

- 완전 연결 신경망
- 모든 노드가 이전 층의 모든 노드와 연결된 신경망 구조
- 입력 데이터의 모든 특징을 학습할 수 있음. 예측 효과적


---

### Fully connected layer

- 완전 연결 계층

    [Google Colab](https://colab.research.google.com/drive/1KWy9u-YMAft1n-To9buu0oKoNMk585Ym?usp=sharing)

---

### Loss Function, 역전파, 옵티마이저

### Loss function(손실 함수)

- 예측값과 실제 값 간의 차이를 정량적으로 측정하는 함수
- 손실 함수를 최소화하는 방향으로 모델 업데이트가 필요<br><br>

종류

1. 회귀(선형회귀) 문제 : 입력 → 출력을 원하는 경우
    1. MSE(평균제곱오차) : 차이 제곱 후 평균
    2. MAE(평균절대오차) : 차이 절댓값 평균 <br><br>
2. 분류(classification) 문제 : 분류 예측 결과를 확률로 대답
    1. Cross-Entropy Loss(크로스 엔트로피 손실)
        
        이항 분류 문제는 베르누이 분포를 따른다고 가정함
        
        모델 예측 확률과 실제 레이블 간 차이를 크로스 엔트로피로 측정
        
    2. Hinge Loss(힌지 손실)
        
        SVM(서포트 벡터 머신) 에서 사용되는 손실 함수
        
        SVM : 데이터를 최적으로 분리하는 초평면을 찾는 지도 학습 알고리즘(경계면)
        
        경계면 기준으로 예측이 멀리 떨어져 있으면 (즉 안전한 구간이면) 손실이 0, 가까우면 큰 손실을 줌<br><br>
        

---

### Backpropagation(역전파)

- 가중치 조정을 위한 기울기, 즉 ‘방향’ 을 계산하는 알고리즘
- 가중치 업데이트는 옵티마이저가 함
    - 기울기를 직접 조정하는 것이 아닌, 가중치를 조정해서 기울기를 자연스럽게 조정
    

체인 룰

- 출력값이 변하면 그 영향이 은닉층을 거쳐 입력층까지 어떻게 전달되는지를 추적하는 방식
- 각 층의 가중치가 최종 손실에 얼마나 기여했는지 계산하기 어렵기 때문에 chain rule 사용해서 출력→은닉→입력 방향으로 미분 값 전파

---

### Optimizer(옵티마이저)

- 손실 함수를 최소화하기 위해 기울기를 기반으로 가중치 업데이트하는 알고리즘
- 최적의 가중치 값 찾는 것 → 최소의 손실 함수값을 위함<br><br>

종류

1. 경사 하강법
    
    가장 기본적인 옵티마이저이며 대표적인 최적화 알고리즘임
    
    - 배치 경사 하강법, 미니 배치 경사 하강법, 확률적 경사 하강법(SGD)이 있음
2. 적응형 옵티마이저(adaptive optimizer)
    
    학습률을 동적으로 조정하여 학습 효율을 높여주는 옵티마이저
    
    - Adagrad : 자주 발생하는 파라미터 업데이트에 대해 학습률을 낮추고, 드물게 발생하는 업데이트에 대해 학습률을 높임
    - RMSprop : 학습률을 조정하면서 과거의 기울기를 반영하여 업데이트
    - ADAM : 모멘텀과 RMSprop의 장점을 결합하여 학습률을 조정. 가장 널리 사용됨
3. 모멘텀 옵티마이저
    
    기울기 벡터의 지수 이동 평균을 사용하여 가중치 업데이트
    
    - NAG : 모멘텀 확장 버전으로, 기울기를 미리 계산하여 더 정확한 업데이트를 가능하게 함

---

### Gradient Descent(경사 하강법)

- 손실 함수를 최소화하기 위해 가중치를 반복적으로 조정하는 최적화 알고리즘
    
<br><br>  

기울기 소실 (Vanishing Gradient)

- 1보다 작은 기울기 값들은 곱할수록 작아진다
- 역전파 과정에서 발생하며, 깊은 층으로 갈수록 기울기가 점점 작아져서 가중치 업데이트가 거의 발생하지 않는 현상을 말함
- 해결방법 : ReLU, 그러나 값이 0보다 작은 경우는 0을 출력하기 때문에 뉴런이 활성화되지 않고 죽어서 영향을 끼치지 못 하는 단점이 있음

---

### Adam

- adaptice moment estimation → adam
- 학습 과정에서 각 파라미터의 학습률을 자동으로 조정함
- 1차 모멘텀(그래디언트 평균) 과 2차 모멘텀(그래디언트 제곱 평균) 을 동시에 추정하여 학습 가속화 및 안정화
<br><br>

---

### CNN (합성곱 인공 신경망)

### CNN(convolutional neural network)

- 여러 개의 convolution layer, pooling layer, fully connected layer들로 구성된 신경망
- 이미지 처리
    - 사진 데이터는 3가지 차원 데이터임(높이, 너비, 색상 채널)
    - fully connected nn에 입력하기 위해서는 평탄화가 필요힌데, 이 과정에서 이미지 공간 정보가 손실됨
    - 이 문제를 해결하기 위해 공간 정보를 유지하면서 학습할 수 있는 모델이 cnn임


- 즉, 이미지와 같은 데이터의 공간적인 구조를 유지하여 특징을 효과적으로 추출함 (필터를 사용해 자동으로 중요한 특징을 학습하고 추출)
- 지역 연결과 가중치 공유를 통해 파라미터 수를 줄여서 계산 효율성을 높임
<br><br>

<aside>
💡

**CNN 입력층**

28x28 이미지를 **28x28 그대로 두고**, 각 픽셀 하나하나를 각 노드에 대응시킴 (각 노드는 하나의 입력값만 받을 수 있음)

즉 노드 하나당 한 픽셀 값만 가짐

**다만, 노드들이 2D 격자 형태(28x28)로 “배치”되어 있음**

(x1 = 0,0 , x2 = 0,1 …. x28 = 0,27 , x29 = 1.0 이런 식으로)

</aside>



---

### Convolutional Layer(합성곱 계층)


이미지의 일부를 값으로 바꾸는 과정

mask(필터)와 이미지 해당 부분에 대한 곱을 각각 하고, 다 합해서 특징을 값으로 추출함 (예제에서는 1+1+1 = 3)

cnn에서 필터는 weight와 동일함

즉 9개의 픽셀을 하나의 값인 3으로 표현<br><br>



<aside>
💡

**공간 정보 유지**

cnn에서는 필터를 사용해 작은 영역을 순차적으로 훑으면서 특징을 추출함. 이 과정에서 특징 추출 뿐 아니라 공간 정보 (즉 이미지 내에서 특징의 ‘위치 정보’) 도 유지됨

이유 : 컨볼루션 결과는 특징 맵(map)으로 저장되는데, 배열 크기만 줄었지 필터가 지나간 위치대로 값이 기록되기 때문. 쉽게 말하면 이미지의 2차원 구조를 그대로 유지한 채 학습하기 때문이다.

</aside>

---

### Pooling

- 특징을 줄여서 계산량을 줄이고, 지나치게 과적합 되는 것을 방지
- 컨볼루션된 결과를 또 줄이기 위해 사용
    
- 분할 후 각 구역의 대표값을 뽑는 것

종류

1. Max pooling : 영역 내에서 가장 큰 값을 대표값으로 정하는 방식
    - 강력한 엣지, 경계 등 두드러지는 값을 강조함
        
    - 노이즈 영향이 상대적으로 작음
    - 대신 극단값에만 집중될 수 있음
2. Average Pooling : 영역 내 모든 값을 더해 평균 내는 방식
    - 평준화된 특성 반영, 결과를 안정적으로 만듦
    - 뚜렷하게 부각해야 하는 특징이 있는 경우 효과적이지 못함<br><br>

---

### Flatten Layer(평탄화 계층)

- 다차원 배열 입력 데이터를 1차원 배열로 변환하여 사용할 수 있도록 하는 layer
        
- Flatten Layer는 다차원 데이터(4D: Batch, Height, Width, Channels)를 Conv2D, MaxPooling2D 레이어를 거쳐 2D 텐서(Batch, Features)로 변환하여, 각 샘플을 1차원 벡터로 만들어 Fully Connected(Dense) Layer에 입력할 수 있도록 한다.
<br><br>
샘플

- cnn에서 각 샘플은 하나의 이미지
- 배치 사이즈가 32라면, 한번 훈련에 32개 이미지를 처리하는 것을 말함
- flatten 적용하면 각 샘플 이미지 하나하나가 특징 벡터가 됨 (1D 벡터로 변환됨)<br><br>

1. **입력 데이터 (이미지들)**
    
    ```python
    (Batch, Height, Width, Channels)
    (32, 28, 28, 1)  # 배치 크기 32, 이미지 크기 28×28, 채널 1 (Grayscale)
    ```
    
2. **Conv2D & Pooling 연산 후**
    
    ```python
    (32, 14, 14, 8)  # 8개의 필터를 적용한 14×14 크기의 특징 맵
    ```
    
3. **Flatten 적용 후**
    
    ```python
    (32, 14×14×8)
    (32, 1568)  # 각 샘플(이미지)이 1568개의 Feature로 표현됨
    ```
    
<br><br>
**원본을 그대로 Flatten 하게 되면, 원본 데이터에 있던 위치정보(x, y등) 같은 특성들이 사라지게 됨**

**따라서 Convolution Layer와 Pooling Layer를 거치면서 특성정보를 보존한 데이터 (고수준 특징맵 )를 Flatten 해서 FCL에서 사용**
<br><br>
### 한 줄 정리

| 개념 | 정리 |
| --- | --- |
| FCNN (Fully Connected Neural Network) | 모든 뉴런이 완전히 연결되어 있어 입력 데이터의 모든 특징을 학습할 수 있는 신경망 구조 |
| 역전파 (Backpropagation) | 손실 함수의 기울기를 계산하여 각 파라미터에 전달하는 알고리즘 |
| 옵티마이저 (Optimizer, 최적화) | 계산된 기울기를 기반으로 모델의 파라미터를 업데이트하여 손실을 최소화하는 알고리즘 |
| 기울기 소실 (Vanishing Gradient) | 역전파 과정에서 기울기가 점점 작아져 깊은 층으로 갈수록 가중치 업데이트가 거의 발생하지 않는 현상 |
| Adam (Adaptive Moment Estimation) | 모멘텀과 RMSProp을 결합해 각 파라미터별로 학습률을 적응적으로 조정하는 최적화 알고리즘 |
| CNN (Convolutional Neural Network) | 합성곱 연산을 통해 이미지의 특징을 추출하고 공간 정보를 유지하는 이미지 처리 신경망 |
| 합성곱 계층 (Convolutional Layer) | convolution mask(필터)를 사용해 입력된 이미지의 국소적인 영역에서 특징을 추출하는 층 |
| 풀링 계층 (Pooling Later) | 분할 후 각 구역 대표값을 뽑는 과정을 통해 중요 특징을 요약하고 공간 크기를 축소시켜 연산량을 줄이는 층 |



---
  
### 💭 오늘의 회고
- 성공적인 점 : CNN 구조에 대해 대략적으로 이해 완료
- 개선해야 할 점 : 해커톤 전 체력 관리
  
### 📁 참고 자료 및 링크
- Alex 강의
