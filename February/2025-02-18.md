## 📅 날짜: 2025-02-18

### 💬 스크럼
- 학습 목표 1 : 데이터 전처리, 머신러닝, 딥러닝 기초 및 pytorch 실습
- 학습 목표 2 : 미니퀘스트 및 강의 정리
  
### 📒 공부한 내용
---
### 인공지능 본질<br><br>

딥러닝의 본질

딥 러닝은 결국 모델 전체를 정의할 수 있는 하나의 거대한 ‘함수’를 찾는 과정이다.

- 해당 함수는 인간이 절대 직관적으로 이해하지 못하는 비선형 복합 함수임
- 쉽게 예를 들면, $f = g_1(g_2(g_3(wx + b)))$ 같은 느낌이다

경사 하강법은 그 전체 함수의 손실값을 최소로 하는 부분을 찾는 과정이다

- 입력값이 두 개 뿐인 경우, 함수의 최솟값은 공간으로 표현할 수 있다
- 이를 예를 들면, 우주에서 블랙홀 주변 공간이 푹 꺼진 상황이라고 할 수 있고, 이 푹 꺼진 지점(최솟값)을 찾는 과정이 경사 하강법인 것이다.

chat gpt 같은 거대 언어 모델도 결국은 하나의 복합 함수라고 정의할 수 있다<br><br>

인공지능 분야는 개발이 아닌 연구 분야임

- 데이터 및 알고리즘에 따라 다양한 방식으로 모델 개선 가능. 즉 내가 아는 방법이 최적이 아닐수도 있음을 늘 염두
- 같은 문제라도 환경에 따라 결과가 달라질 수 있음 (예측이 100퍼센트 정확하지 않음)<br><br>

### 데이터 전처리

Tensorflow : 텐서(데이터의 단위)가 흘러다니면서 처리가 된다는 뜻. 구글에서 개발한 오픈소스 ML/DL 라이브러리

Keras : 텐서플로우를 쉽게 사용할 수 있도록 돕는 라이브러리. 

2017년 이후 텐서플로우에서 자체적으로 keras를 포함해서 고수준 API로 제공하기 시작함

PyTorch : 연구, 실무에서 주로 사용되는 강력한 딥러닝 설계 라이브러리. 현재는 텐서플로우보다 더 많이 사용함. layer 쌓고 모델 만드는게 깔끔함

- Autograd 모듈을 통해 연산 그래프를 자동으로 구성하고, 이를 기반으로 역전파를 수행하여 기울기를 쉽게 계산<br><br>

벡터 : 다차원 공간에서 방향과 크기를 가지며, 입력 데이터를 표현하는 데 사용<br><br>

벡터 : 1차원 배열 (단어 임베딩, 픽셀 값)

행렬 : 2차원 배열 (이미지 데이터, 연립 방정식의 계수 행렬)

텐서 : 다차원 배열, 여러 차원 포함 (이미지, 비디오, 음성 데이터)

---

데이터 전처리 : 데이터 분석과 모델 학습을 위해 원본 데이터를 정제하고 변환하는 과정. 즉 모델 성능 향상을 위해 더 좋은 데이터를 넣는 과정
<br><br>
종류

- 결측값 처리 : 누락 데이터 식별 및 대체
- 이상치 제거 : 비정상적 값들 식별 및 제거
- 데이터 정규화 : 데이터를 일정한 범위로 변환
- 데이터 변환 : 로그 변환, 원-핫 인코딩 등<br><br>

- 원-핫 인코딩<br><br>
    
    범주형 데이터를 이진 벡터(이진법으로만 이루어진 벡터)로 변환하는 기법
    
    - 범주형 변수는 모델이 해석하지 못하기 때문에 수치형으로 변환해야 함
    - 범주는 순서가 없는 경우가 많기 때문에 단순히 1,2,3,4,..등으로 매핑은 불가능
    - 따라서 각 범주를 고유한 이진 벡터로 변환해야 함
    
    
    
    → 이진수에 대해 자릿수가 있음. 사과를 나타내고 싶으면 사과 자릿수만 1이고 나머지는 0 (사과 = [1, 0, 0])
    
    → 데이터가 낭비되는 단점이 있지만 유용한 경우가 있어 사용<br><br>
    
- 이미지 전처리<br><br>
    
    픽셀 별로 RGB값이 얼마인지를 벡터로 변환<br><br>
    
- 스케일링(정규화)<br><br>
    
    동일한 범위로 단위 변환. 정규화, 표준화가 있음.
    
    - 한 변수가 0~1 범위고, 다른 변수는 0~10000인 경우 범위를 맞춰줌
    
    ```mermaid
    graph LR
        A[원본 데이터] --> B{스케일링 방법}
        B -->|표준화| C[표준화 과정]
        B -->|정규화| D[정규화 과정]
        
        C --> E[평균 = 0, 표준편차 = 1]
        C --> F["(x - 평균) / 표준편차"]
        
        D --> G[특정 범위로 변환 ex 0~1]
        D --> H["(x - min) / (max - min)"]
        
        E --> I[표준화된 데이터]
        F --> I
        
        G --> J[정규화된 데이터]
        H --> J
        
        I --> K[스케일링된 데이터]
        J --> K
        
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style B fill:#bbf,stroke:#333,stroke-width:2px
        style C fill:#dfd,stroke:#333,stroke-width:2px
        style D fill:#dfd,stroke:#333,stroke-width:2px
        style E fill:#afa,stroke:#333,stroke-width:2px
        style F fill:#afa,stroke:#333,stroke-width:2px
        style G fill:#fda,stroke:#333,stroke-width:2px
        style H fill:#fda,stroke:#333,stroke-width:2px
        style I fill:#aff,stroke:#333,stroke-width:2px
        style J fill:#aff,stroke:#333,stroke-width:2px
        style K fill:#f9f,stroke:#333,stroke-width:2px
    ```
    <br><br>
    
- 데이터 분할(split)<br><br>
    
    데이터를 학습용과 검증용으로 나누는 것
    
    ```python
    from sklearn.model_selection import train_test_split
    
    # 데이터 분할 (학습용 80%, 검증용 20%)
    train, test = train_test_split(df, test_size=0.2, random_state=42)
    ```
    
    - random_state 를 사용하여 8:2로 무작위 분할<br><br>

---

데이터 증강 : 기존 데이터를 기반으로 새로운 데이터 생성(합성 데이터), 변형에 조금 더 가까움

- 이미지 회전
- 크기 조절
- 뒤집기
- 색 변형
- 노이즈 추가 - 랜덤 픽셀 변화 추가
- 역변역 - 문장을 다른 언어로 번역한 후 다시 원래 언어로 번역
- 속도 변화 - 오디오 파일 속도 변경<br><br>

사용 이유 : 데이터 다양성과 양을 증가시켜 과적합 방지 및 성능 향상<br><br>

한계

- 실제 데이터와 완전히 동일하지 않아 학습 왜곡될 수도 있음
- 합성 데이터가 너무 많으면 오히려 합성 데이터에 의존해 과적합 발생 가능
    - 과적합 : 학습 데이터에 너무 특화되어 실제 데이터(새로운 데이터) 예측을 못함. 즉 특정 케이스에만 특화된 것
- 이미지, 음성 분야에는 특화적이나 금융, 정형 데이터 경우는 제한적임<br><br>

---

데이터셋 분할



- 학습 데이터 : 모델이 패턴을 학습하는 데 사용되는 데이터
- 테스트 데이터 : 학습 완료된 모델 최종 성능 평가하는 데이터 (한 번도 본 적 없는 데이터로 평가)
- 검증(validation) 데이터 : 하이퍼파라미터 튜닝 및 모델 최적화 과정에서 사용되는 데이터 (한 번도 본적 없는 데이터로 학습 중에 평가)
<br><br>
파라미터 : w(가중치), b(바이어스) 등 모델 자체 값. 모델이 학습을 통해 자동으로 조정

하이퍼파라미터 : a(학습률) 등 모델 학습 과정에서 필요한 값. 학습 전 사용자가 직접 설정

- 모델 구조 : 은닉층 개수, 각 층 뉴런 수, 활성화 함수(ReLU 등)
- 학습 과정 : 학습률(파라미터 업데이트 할 때 한 번에 이동하는 크기), 에포크 수(학습 횟수), 최적화 알고리즘(옵티마이저 - Adam 등)
<br><br>
활성화 함수

- 각 뉴런에 적용되어 선형 변환만으로는 표현할 수 없는 비선형성을 모델에 추가함
- 입력값에 대해 단순히 w를 곱하고 b를 더하는 선형 변환만 있다면 모델 전체가 결국 하나의 선형 함수가 되어 복잡한 데이터 패턴 학습을 못 함
- 예) ReLU (음수는 0, 양수는 그대로 전달, 실제 뉴런 방식과 유사)
<br><br>
최적화 알고리즘

- 모델 전체의 w, b를 최적의 값으로 조정하는 과정
- 역전파를 사용해서 손실 함수의 기울기를 계산하고, 경사하강법과 그 변형을 적용하여 파라미터 업데이트
- 예) Adam (경사하강법 개선법. 과거 기울기의 지수이동평균 기억해서 업데이트에 반영)

### 머신러닝

머신 러닝 : 데이터를 보고 패턴을 학습하여 예측하는 알고리즘 분야, 즉 스스로 하는 학습<br><br>



- 예전 : 프로그래밍을 직접 짜서 컴퓨터한테 일을 시킴 (직접 하나하나 다 알려줌)
- now : 명시적인 프로그래밍 없이도 데이터를 통해 학습하고 예측을 개선<br><br>

머신러닝 단계

- 데이터 수집
- 데이터 전처리
- 데이터 분할
- 모델 선택 및 훈련
- 모델 평가
- 모델 조정 및 재훈련
- 테스트 데이터로 예측
- 모델 예측 결과 확인
- 최종 모델 확정<br><br>

단순한 숫자 예측 모델

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 데이터 수집
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]) 

# print(X)
# print(y)
# 모델 선택 및 학습
model = LinearRegression()
model.fit(X, y) # fit은 모델을 데이터에 맞추는(훈련하는) 함수

# 예측 및 평가
X_test = np.array([[11], [12], [13], [14], [15]])
# model.predict(X_test)는 학습된 선형 회귀 모델을 사용하여 새로운 데이터 X_test에 대한 예측값을 계산하는 함수
y_pred = model.predict(X_test) 

print("예측값:", y_pred) 
```

- 선형 회귀 모델을 불러와 학습시킴 (fit 함수로 학습)

### 딥러닝

딥 러닝(DL) : 대량의 데이터를 기반으로 ‘비선형 모델’을 자동으로 만들어주는 기법, ANN 구조(인공신경망)<br><br>

- 선형 : 직선
- 비선형 : 곡선 ($y = x^2$ 등)
- 선형 : 연산의 단순화, 근사를 위해 사용
- 미분은 어떤 함수의 **국소적인 선형 근사**를 제공하지만, 이 자체가 세상의 비선형성을 제거하는 것은 아니다.
- 모델이 하나의 거대한 선형 함수가 되는 것을 방지하기 위해 비선형성(활성화 함수)을 추가함<br><br>

딥러닝 단계

- 데이터 준비(전처리 등)
- 모델 설계
- 모델 초기화 - 가중치와 바이어스 초기화
- 훈련
- 평가
- 하이퍼파라미터 조정 - 활성화 함수, 최적화 알고리즘, 학습률, epoch 수 등
- 배포<br><br>

하이퍼파라미터 종류

- 학습률 : 모델이 가중치를 업데이트 하는 속도를 결정하는 값
- 배치 크기 : 한 번 학습에 사용되는 훈련 데이터 크기
- epoch : 학습 횟수
- layer : 신경망 층 수
- 드롭아웃 비율 : 일부 뉴런을 무작위로 제외(과적합 방지)<br><br>

XOR 예제

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# XOR 데이터 정의
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y = np.array([[0], [1], [1], [0]], dtype=np.float32)

# Tensor로 변환
X = torch.tensor(X)
y = torch.tensor(y)

class XORModel(nn.Module):
    def __init__(self):
        super(XORModel, self).__init__()  # nn.model(부모) 초기화
        self.layer1 = nn.Linear(2, 2)  # 첫 번째 선형 레이어, 입력 크기 2, 출력 크기 2
        self.layer2 = nn.Linear(2, 1)  # 두 번째 선형 레이어, 입력 크기 2, 출력 크기 1
        self.relu = nn.ReLU()  # ReLU 활성화 함수
        self.sigmoid = nn.Sigmoid()  # Sigmoid 활성화 함수

    def forward(self, x):
        x = self.relu(self.layer1(x))  # 첫 번째 레이어와 ReLU 적용
        x = self.sigmoid(self.layer2(x))  # 두 번째 레이어와 Sigmoid 적용
        return x

model = XORModel()  # XORModel 클래스의 인스턴스 생성

criterion = nn.BCELoss()  # binary_crossentropy와 동일
optimizer = optim.Adam(model.parameters(), lr=0.01)
num_epochs = 1000  # 총 에포크 수

for epoch in range(num_epochs):
    model.train()  # 모델을 훈련 모드로 설정
    optimizer.zero_grad()  # 옵티마이저의 변화도(gradient)를 초기화
    outputs = model(X)  # 모델에 입력 데이터를 넣어 출력 계산
    loss = criterion(outputs, y)  # 출력과 실제 레이블을 비교하여 손실 계산
    loss.backward()  # 역전파를 통해 손실에 대한 그래디언트 계산
    optimizer.step()  # 옵티마이저가 매개변수를 업데이트

    if (epoch + 1) % 100 == 0:  # 100 에포크마다 손실 출력
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

model.eval()  # 모델을 평가 모드로 전환
with torch.no_grad():  # 평가 중에는 그래디언트를 계산하지 않음
    outputs = model(X)  # 모델에 입력 데이터를 전달하여 출력값 계산
    predicted = (outputs > 0.5).float()  # 출력값이 0.5보다 크면 1, 아니면 0으로 변환 (이진 분류)
    accuracy = (predicted == y).float().mean()  # 예측값과 실제값을 비교하여 정확도 계산
    loss = criterion(outputs, y)  # 손실 함수(크로스 엔트로피 손실)를 사용하여 손실 계산
    print(f'Loss: {loss.item()}, Accuracy: {accuracy.item()}')  # 손실과 정확도 출력
    
    
with torch.no_grad():
    predictions = model(X)
    print(f'Predictions: {predictions}')
```

### 퍼셉트론

퍼셉트론 : ANN(인공신경망)의 기본 단위, 입력값을 가중치와 함께 처리하여 단일 출력을 생성하는 선형 이진 분류기 (생물학적 뉴런과 비슷)

- 함수 내부에 조건문이 있는 함수.


가중합 : 입력값(벡터)과 가중치의 선형 결합 (스칼라 곱, 벡터 합 - 즉 ax + b)

가중합 + 활성화 함수 = 출력값 (새로운 퍼셉트론의 입력값)<br><br>

활성화 함수의 차이

- 단층 퍼셉트론 : 계단 함수
- 다층 퍼셉트론(MLP) : 시그모이드, ReLU<br><br>

가중치와 편향 초기화 이유

- 랜덤값으로 설정 - 대칭 깨기(symmetry breaking) 때문임
- 모든 가중치를 0 또는 동일한 값으로 초기화하면, 모든 뉴런이 동일하게 업데이트되어 학습이 진행이 안됨<br><br>

퍼셉트론 학습 과정 (딥러닝 학습의 본질 - 행렬 내적(곱셈))

```python
for epoch in range(epochs):
    for i in range(len(inputs)):
        # 총 입력 계산
        total_input = np.dot(inputs[i], weights) + bias
        # 예측 출력 계산
        prediction = step_function(total_input)
        # 오차 계산
        error = outputs[i] - prediction
        # 가중치와 편향 업데이트
        weights += learning_rate * error * inputs[i]
        bias += learning_rate * error
```

- 코드를 잘 보면 가중치를 기억하지 않음
- epoch가 20이고 입력 4개라면 총 80번 iteration. 가중치 80개는 기억되는게 아닌 계속 누적된다.<br><br>

딥러닝과 퍼셉트론 둘다 가중치가 누적됨

퍼셉트론은 ‘가중치 업데이트 결과’가 누적되는 것이고 딥러닝에서는 계산된 기울기(그래디언트) 가 누적되지 않도록 매번 초기화하나 ‘가중치 업데이트 결과 자체’는 누적됨
<br><br>
이 과정이 반복되면 알아서 최적의 값을 잘 찾아가는 이유 :

- 역전파 알고리즘이 오차를 각 파라미터까지 전파하여 영향을 계산하고 그래디언트를 구함
- 경사 하강법 알고리즘이 해당 그래디언트의 반대 방향으로 가중치를 업데이트하여 결국 최소값에 도달하게 됨

### 한 줄 정리

한 줄 정리<br><br>

파이토치 : 연구, 실무에서 주로 사용되는 강력한 딥러닝 프레임워크

데이터 전처리 : 원본 데이터를 정제, 변환, 정규화하여 학습에 최적의 상태로 만드는 것. 분석과 학습 성능 향상을 위해 사용

원-핫 인코딩 : 범주형 데이터를 모델이 해석할 수 있도록 독립적인 이진 벡터로 변환하는 기법

데이터 증강 : 기존 데이터를 변형하여 합성 데이터를 생성하는 과정이며 데이터가 다양해져 과적합을 방지함

데이터셋 분할 : 데이터를 학습, 검증, 테스트용으로 나누는 것. 모델 학습, 하이퍼파라미터 튜닝, 최종 모델 평가를 독립적으로 진행하기 위해 사용

머신 러닝 : 데이터를 보고 패턴을 학습하여 예측하는 알고리즘 분야

과적합 : 모델이 학습 데이터에 지나치게 맞춰져 새로운 데이터에 대해 성능이 떨어지는 현상

딥 러닝 :  대량의 데이터를 다층 신경망을 통해 학습하는 머신러닝의 한 분야

퍼셉트론 : 인공 신경망의 기본 단위이며 가중합과 활성화 함수로 구성됨


  
### 📁 참고 자료 및 링크
- Alex 강의
