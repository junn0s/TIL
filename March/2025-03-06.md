## 📅 날짜: 2025-03-06

### 💬 스크럼
- 학습 목표 1 : Zero-Shot Learning
- 학습 목표 2 : TensorBoard, 정규화 기법
  
### 📒 공부한 내용
---

딥러닝 구조 전반적 이미지 <br><br>

최소, 최대값을 찾는 게 아닌, 그 값이 되도록 하는 argument(인수)를 찾는 것이 목표

---

Zero-Shot Learning - 학습한 적이 없어도 새로운 개념 생성 가능

Few-Shot Learning - 적은 양의 학습 데이터로도 일반화 능력을 갖도록 하는 기술

---

TensorBoard : 학습 과정을 실시간으로 시각화<br><br>

- 사용 방법 (PyTorch)
    1. **필요한 라이브러리 임포트 및 설정**
        
        PyTorch, TensorBoard 등을 임포트하고 로그를 저장할 디렉토리를 설정합니다.
        
        ```python
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.tensorboard import SummaryWriter
        import torchvision
        import torchvision.transforms as transforms
        import datetime
        
        # 현재 시간을 기반으로 로그 디렉토리를 생성합니다.
        log_dir = "logs/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # SummaryWriter를 사용해 TensorBoard에 기록할 객체를 만듭니다.
        writer = SummaryWriter(log_dir=log_dir)
        ```
        
    2. **모델 학습 과정에서 TensorBoard에 기록하기**
        
        학습 루프 내부에서 손실(loss), 정확도(accuracy) 등 지표를 계산한 뒤 `add_scalar()` 메서드를 통해 로그로 기록합니다.
        
        ```python
        for epoch in range(num_epochs):
            running_loss = 0.0
            correct, total = 0, 0
        
            # 배치 단위로 데이터를 가져와서 모델에 입력합니다.
            for i, (inputs, labels) in enumerate(trainloader):
                # 옵티마이저의 기울기(gradient)를 초기화합니다.
                optimizer.zero_grad()
                
                # 모델에 입력을 전달하여 순전파(forward)를 수행합니다.
                outputs = model(inputs)
                
                # 예측값(outputs)과 실제값(labels)을 비교해 손실(loss)을 계산합니다.
                loss = criterion(outputs, labels)
                
                # 역전파(backward)를 통해 가중치에 대한 그래디언트를 계산합니다.
                loss.backward()
                
                # 옵티마이저로 가중치를 업데이트합니다.
                optimizer.step()
        
                # 배치별 손실값을 running_loss에 계속 더해줍니다.
                running_loss += loss.item()
                
                # 배치 단위의 예측값을 도출하고, 정확도를 계산하기 위한 지표를 누적합니다.
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
            # 한 에폭이 끝난 후, 전체 학습 데이터에 대한 평균 손실과 정확도를 계산합니다.
            train_accuracy = correct / total
            train_loss = running_loss / len(trainloader)
            
            # TensorBoard에 학습 손실과 정확도를 기록합니다.
            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Accuracy/train", train_accuracy, epoch)
        ```
        
    
- 적용 예시 (PyTorch)
    
    ```python
    import torch  # PyTorch 라이브러리 불러오기
    import torch.nn as nn  # 신경망 모듈 불러오기
    import torch.optim as optim  # 최적화 알고리즘 불러오기
    import torchvision  # 이미지 처리 관련 라이브러리 불러오기
    import torchvision.transforms as transforms  # 데이터 변환을 위한 모듈 불러오기
    import matplotlib.pyplot as plt  # 그래프를 그리기 위한 라이브러리
    import numpy as np  # 수학 연산을 위한 라이브러리
    from torch.utils.data import DataLoader  # 데이터 로더 모듈 불러오기
    import time  # 시간 측정을 위한 라이브러리
    import IPython.display as display  # IPython에서 디스플레이 기능 활용
    from IPython.display import clear_output  # 화면을 지우기 위한 모듈
    
    # 데이터 로드 및 전처리 설정
    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),  # 랜덤으로 좌우 반전 적용
        transforms.RandomRotation(10),  # 랜덤으로 ±10도 회전 적용
        transforms.RandomAffine(degrees=0, shear=10, scale=(0.8, 1.2)),  # 랜덤 변형 (이동, 확대/축소, 기울기)
        transforms.ToTensor(),  # 이미지를 텐서 형태로 변환
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 이미지 정규화 (평균 0, 표준편차 1)
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),  # 테스트 데이터도 텐서 변환
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 동일한 정규화 적용
    ])
    
    # CIFAR-10 데이터셋 다운로드 및 로드
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    
    # 데이터 로더 생성 (배치 크기 32)
    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)  # 훈련 데이터 로더
    testloader = DataLoader(testset, batch_size=32, shuffle=False)  # 테스트 데이터 로더
    
    # CNN 모델 정의
    class CNNModel(nn.Module):
        def __init__(self):
            super(CNNModel, self).__init__()
            self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # 첫 번째 합성곱층 (입력: RGB 3채널, 출력: 32채널)
            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 2x2 맥스 풀링 적용
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 두 번째 합성곱층 (입력: 32채널, 출력: 64채널)
            self.fc1 = nn.Linear(64 * 8 * 8, 64)  # 첫 번째 완전 연결층 (입력: 64x8x8, 출력: 64)
            self.fc2 = nn.Linear(64, 10)  # 두 번째 완전 연결층 (출력: 10 클래스)
    
        def forward(self, x):
            x = torch.relu(self.conv1(x))  # 첫 번째 합성곱층 + ReLU 활성화 함수 적용
            x = self.pool(x)  # 맥스 풀링 적용
            x = torch.relu(self.conv2(x))  # 두 번째 합성곱층 + ReLU 활성화 함수 적용
            x = self.pool(x)  # 맥스 풀링 적용
            x = x.view(-1, 64 * 8 * 8)  # 1차원 벡터로 변환
            x = torch.relu(self.fc1(x))  # 첫 번째 완전 연결층 + ReLU 활성화 함수 적용
            x = self.fc2(x)  # 두 번째 완전 연결층 (출력층)
            return x
    
    # 실시간 그래프를 그리는 함수
    def plot_live(train_loss, val_loss, train_acc, val_acc):
        clear_output(wait=True)  # 출력 창을 지우기 (새로운 그래프를 위해)
        plt.figure(figsize=(12, 5))  # 그래프 크기 설정
    
        plt.subplot(1, 2, 1)  # 1행 2열의 첫 번째 그래프 (손실값)
        plt.plot(train_loss, label="Train Loss", marker="o")
        plt.plot(val_loss, label="Validation Loss", marker="o")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Loss Over Epochs")
        plt.legend()
    
        plt.subplot(1, 2, 2)  # 1행 2열의 두 번째 그래프 (정확도)
        plt.plot(train_acc, label="Train Accuracy", marker="o")
        plt.plot(val_acc, label="Validation Accuracy", marker="o")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.title("Accuracy Over Epochs")
        plt.legend()
    
        plt.show()  # 그래프 출력
    
    # 모델 학습 함수 정의
    def train_model(model, trainloader, testloader, epochs=20):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # GPU 사용 여부 확인
        model.to(device)  # 모델을 GPU로 이동
    
        criterion = nn.CrossEntropyLoss()  # 손실 함수 (교차 엔트로피)
        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저 설정
    
        train_loss, val_loss = [], []  # 훈련 및 검증 손실 저장 리스트
        train_acc, val_acc = [], []  # 훈련 및 검증 정확도 저장 리스트
    
        for epoch in range(epochs):  # 지정한 에포크 수만큼 반복
            model.train()  # 모델을 훈련 모드로 설정
            running_loss, correct, total = 0.0, 0, 0  # 초기 손실 및 정확도 변수 설정
            for inputs, labels in trainloader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                optimizer.zero_grad()  # 기울기 초기화
                outputs = model(inputs)  # 모델 예측 수행
                loss = criterion(outputs, labels)  # 손실 계산
                loss.backward()  # 역전파 수행
                optimizer.step()  # 가중치 업데이트
                running_loss += loss.item()  # 손실 누적
                _, predicted = torch.max(outputs, 1)  # 예측값 계산
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
    
            train_loss.append(running_loss / len(trainloader))  # 평균 훈련 손실 저장
            train_acc.append(correct / total)  # 훈련 정확도 저장
    
            model.eval()  # 모델을 평가 모드로 설정
            val_running_loss, correct, total = 0.0, 0, 0  # 초기 손실 및 정확도 변수 설정
            with torch.no_grad():
                for inputs, labels in testloader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    val_running_loss += loss.item()
                    _, predicted = torch.max(outputs, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
    
            val_loss.append(val_running_loss / len(testloader))  # 평균 검증 손실 저장
            val_acc.append(correct / total)  # 검증 정확도 저장
    
            plot_live(train_loss, val_loss, train_acc, val_acc)  # 실시간 그래프 업데이트
    
        return train_loss, val_loss, train_acc, val_acc  # 결과 반환
    ```
<br><br>
    

런팟(RunPot) → 대규모 모델 학습, gpu 선택 가능

---

정규화 기법

| 정규화 기법 | 설명 | 작동 방식 | 특징 |
| --- | --- | --- | --- |
| **L1 정규화 (Lasso 정규화)** | 가중치의 절대값 합에 비례하는 패널티를 추가 | 가중치를 0으로 만들어 모델을 희소하게 함 | 중요하지 않은 특징을 제거, 모델의 해석 가능성 높음 |
| **L2 정규화 (Ridge 정규화)** | 가중치의 제곱합에 비례하는 패널티를 추가 | 모든 가중치를 작게 만들어 모델의 복잡도를 줄임 | 모든 특징이 일부 영향을 미침, 부드러운 모델 생성, 오버피팅 방지 |
| **엘라스틱넷 정규화 (Elastic Net)** | L1과 L2 정규화를 동시에 사용 | L1과 L2 정규화의 장점을 결합 | 과적합을 효과적으로 방지 |
| **Dropout** | 딥러닝 모델 학습 중 무작위로 뉴런을 비활성화하여 과적합 방지 | 각 훈련 단계에서 특정 확률(p)로 뉴런을 비활성화 | 특정 뉴런이나 경로에 의존하지 않게 하여 네트워크가 더욱 견고해짐 |


---
  
### 📁 참고 자료 및 링크
- Alex 강의
