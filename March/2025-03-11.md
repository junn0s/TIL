## 📅 날짜: 2025-03-11

### 💬 스크럼
- 학습 목표 1 : LangChain, LLM Chain
- 학습 목표 2 : RAG, Embedding, 유사도
  
### 📒 공부한 내용
---
### LangChain

프로젝트 역할 분담

- AI 개발자 : 모델을 활용하여 랭체인으로 LLM 애플리케이션 제작
- AI 모델러 : 모델 설계, 학습, 파인튜닝 등

<br><br>
PyTorch : 기반이 되는 프레임워크

LangChain : LLM 사용해서 애플리케이션 생성을 단순화하도록 설계된 프레임워크 (RAG)

HuggingFace : LLM(트랜스포머), 데이터셋 제공

---
### RAG

Retrieval : 기존 데이터베이스나 문서에서 관련 정보를 검색하는 것

Retrieval Augmented Generation (RAG) : 대규모 언어 모델 출력을 최적화하여 , 응답 생성 전에 학습 데이터 소스 **외부의 지식 베이스를 참조**하도록 하는 프로세스

- 즉 외부 지식을 사용해서 직접 검색한 뒤 생성 과정에서 활용하도록 하는 것
- **추가 지식을 넣는 과정**

---

### LLM Chain

LLM을 여러개 연결해서 복잡한 자연어 처리 작업 하는 기술

- 연속된 단계의 프롬포트 탬플릿을 사용하여 LLM 연결

```python
# 첫 번째 프롬프트 (질문 요약)
summary_prompt = PromptTemplate.from_template("질문을 간결하게 요약하세요: {query}")

# 두 번째 프롬프트 (요약된 질문을 바탕으로 분석)
analysis_prompt = PromptTemplate.from_template("이 질문의 핵심 개념을 설명하세요: {summary}")

# 세 번째 프롬프트 (최종 응답 생성)
final_prompt = PromptTemplate.from_template("이 정보를 바탕으로 사용자에게 적절한 답변을 제공하세요: {analysis}")

# 체인 정의
def summarize(query):
    return llm1.invoke(summary_prompt.format(query=query))

def analyze(summary):
    return llm1.invoke(analysis_prompt.format(summary=summary))

def generate_response(analysis):
    return llm1.invoke(final_prompt.format(analysis=analysis))

**# 체인 실행 (RunnableLambda 사용)
chain = RunnableLambda(summarize) | RunnableLambda(analyze) | RunnableLambda(generate_response)**

# 사용자 질문 실행
query = "LLaMA랑 GPT랑 뭐가 더 좋아요? 그리고 LLaMA는 일론 머스크가 만들었나요?"
response = chain.invoke(query)

# 결과 출력
print("\n[LLM을 여러 번 호출하여 생성한 최종 답변]:")
print(response)
```
---

### Embedding, 유사도

Embedding : 자연어를 기계가 이해할 수 있도록 숫자 (벡터) 로 변환하는 것

- 의미 파악과 유사성 계산을 위해 사용함<br><br>

종류

- 단어 임베딩 : 개별 단어를 벡터 변환하여 단어 간 관계 수치 표현
- 문장 임베딩 : 문장의 전체 의미를 하나의 벡터로 변환
- 문서 임베딩 : 전체 문서 내용을 벡터로 변환

---

문서 임베딩<br><br>

TF-IDF : 문서에서 단어의 상대적 중요도를 평가하는 대표적인 벡터화 방법

- 단순히 단어 등장 횟수를 세는 것이 아닌, 특정 문서에서 자주 등장하지만 전체 문서에서는 드문 단어에 더 높은 가중치를 부여하여 의미 있는 단어를 강조함
- 최근 트렌드는 아님 (빈도수 기반이기 때문)<br><br>

BERT : 문맥을 반영하여 단어 의미를 보다 정교하게 표현

- 단어가 사용된 문맥까지 고려
- 예) ‘유리’ 가 명사, 혹은 ‘유리하다’ 뜻 2가지로 작용
- 의미 별로 다르게 해석 가능<br><br>

토큰화 : 문장을 단어 단위로 나누는 과정

불용어 : 빈번하지만 의미가 크지 않은 단어 (the, is, in, and 등등)

---

유사도 검색 : 텍스트 데이터 간의 유사도를 계산하여 유사한 텍스트를 검색하는 기능

- 문자열 자체를 비교하는 것이 아니라, 의미적으로 얼마나 가까운지를 벡터 공간에서 계산함
- 코사인 유사도, 유클리디안 거리 등 거리 계산 방법 사용됨<br><br>

코사인 유사도 : 두 벡터 간의 각도를 이용하여 유사도 측정

유클리디안 거리 : 좌표 간의 실제 거리를 기반으로 유사도 측정<br><br>

쿼리 : 검색, 요청을 위해 입력하는 명령, 문장 (SQL문 등)

유사도 검색 코드

```python
def similarity_search(query, top_n=3, method="cosine"):
    query_vector = embedding_model.embed_query(query)  # 쿼리를 벡터로 변환

    if method == "cosine":
        similarities = [1 - cosine(query_vector, doc_vec) for doc_vec in document_vectors]  # 코사인 유사도 계산
        sorted_indices = np.argsort(similarities)[-top_n:][::-1]  # 높은 유사도 순으로 정렬
    elif method == "euclidean":
        similarities = [euclidean(query_vector, doc_vec) for doc_vec in document_vectors]  # 유클리디안 거리 계산
        sorted_indices = np.argsort(similarities)[:top_n]  # 거리가 짧은 순으로 정렬
    else:
        raise ValueError("지원되지 않는 검색 방식입니다. 'cosine' 또는 'euclidean'을 선택하세요.")

    return [(documents[i], similarities[i]) for i in sorted_indices]
```

---

벡터 스코어 : 고차원 벡터로 표현된 데이터를 관리할 수 있도록 설계된 DB

- 텍스트, 이미지, 오디오 등 다양한 **비정형 데이터**를 수치화된 벡터 형식으로 저장, 가장 유사한 데이터를 빠르게 찾을 수 있게 해줌

---

FAISS : Facebook AI Similarity Search, 고차원 벡터 데이터에서 빠르고 효율적인 유사성 검색을 위한 라이브러리

- 벡터 인덱싱과 ANN(근사 최근접 이웃 탐색) 기법을 활용함




--- 
### 📁 참고 자료 및 링크
- Alex 강의
